{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 11,
=======
   "execution_count": 25,
>>>>>>> 35f3b5f (update)
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本的なライブラリ\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import random\n",
<<<<<<< HEAD
=======
    "import pickle\n",
>>>>>>> 35f3b5f (update)
    "\n",
    "# モデル関連\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
<<<<<<< HEAD
    "\n",
=======
    "from sklearn.linear_model import Ridge, Lasso\n",
>>>>>>> 35f3b5f (update)
    "\n",
    "# モデル評価・前処理関連\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve, cross_val_score, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ハイパーパラメータ最適化関連\n",
    "import optuna\n",
    "from optuna.integration import LightGBMPruningCallback, XGBoostPruningCallback\n",
    "import optuna.visualization as vis\n",
<<<<<<< HEAD
=======
    "import logging\n",
    "import optuna\n",
>>>>>>> 35f3b5f (update)
    "\n",
    "# その他のユーティリティ\n",
    "import shap\n",
    "import joblib\n",
    "\n",
    "# 乱数のシードを固定\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# 警告を無視\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
=======
   "execution_count": 26,
>>>>>>> 35f3b5f (update)
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの読み込み\n",
<<<<<<< HEAD
    "all_data = pd.read_csv('../data_processed/all_data.csv')\n",
=======
    "all_data = pd.read_csv('../../data_processed/all_data.csv')\n",
>>>>>>> 35f3b5f (update)
    "\n",
    "train_data = all_data[0:27532]\n",
    "X = train_data.drop(['price'], axis=1)\n",
    "y = train_data['price']\n",
    "\n",
    "# データの分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # モデルの選択\n",
    "    model_type = trial.suggest_categorical('model_type', ['catboost', 'lightgbm', 'xgboost'])\n",
    "    \n",
    "    # 交差検証の設定\n",
    "    n_splits = 5\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    oof_preds = []\n",
    "    y_vals = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if model_type == 'catboost':\n",
    "            params = {\n",
    "                'depth': trial.suggest_int('depth', 4, 16),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 0.00001, 0.1),\n",
    "                'iterations': trial.suggest_int('iterations', 50, 2000),\n",
    "                'l2_leaf_reg': trial.suggest_int('l2_leaf_reg', 1, 20),\n",
    "                'border_count': trial.suggest_categorical('border_count', [32, 64, 128, 256, 512]),\n",
    "                'thread_count': 4\n",
    "            }\n",
    "            model = CatBoostRegressor(**params, random_seed=42, verbose=0)\n",
    "            model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, \n",
    "                      cat_features=['region', 'manufacturer', 'condition', 'cylinders', 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state'])\n",
    "        \n",
    "        elif model_type == 'lightgbm':\n",
    "            params = {\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 10, 50),\n",
    "                'max_depth': trial.suggest_int('max_depth', -1, 5),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.1, log=True),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 100),\n",
    "                'min_child_samples': trial.suggest_int('min_child_samples', 20, 100),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0, step=0.1),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0, step=0.1),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5, step=0.1),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5, step=0.1),\n",
    "                'force_col_wise': True,  \n",
    "                'verbose': -1\n",
    "            }\n",
    "            model = LGBMRegressor(**params, random_state=42)\n",
    "            model.fit(X_train, y_train, eval_metric='mape', eval_set=[(X_val, y_val)],\n",
    "                      callbacks=[LightGBMPruningCallback(trial, 'l2')])\n",
    "        \n",
    "        else:  # xgboost\n",
    "            params = {\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 9),\n",
    "                'learning_rate': trial.suggest_loguniform('learning_rate', 0.001, 0.1),\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "                'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 0.0, 0.5),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 0.0, 0.5),\n",
    "                'verbosity': 0,\n",
    "                'objective': 'reg:squarederror',\n",
    "                'booster': trial.suggest_categorical('booster', ['gbtree', 'gblinear', 'dart']),\n",
    "                'tree_method': 'hist',\n",
    "                'seed': 42\n",
    "            }\n",
    "            dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "            dval = xgb.DMatrix(X_val, label=y_val)\n",
    "            model = xgb.train(params, dtrain, evals=[(dval, 'eval')], early_stopping_rounds=10, \n",
    "                      callbacks=[XGBoostPruningCallback(trial, 'eval-rmse')], verbose_eval=False)\n",
    "        \n",
    "        if model_type == 'xgboost':\n",
    "            preds = model.predict(xgb.DMatrix(X_val))\n",
    "        else:\n",
    "            preds = model.predict(X_val)\n",
    "        \n",
    "        oof_preds.append(preds)\n",
    "        y_vals.append(y_val)\n",
    "    \n",
    "    # OOF予測をスタッキングの入力として使用\n",
    "    oof_preds = np.concatenate(oof_preds, axis=0)\n",
    "    y_vals = np.concatenate(y_vals, axis=0)\n",
    "    \n",
    "    # メタモデルの選択\n",
    "    meta_model_type = trial.suggest_categorical('meta_model_type', ['random_forest', 'ridge', 'gbm'])\n",
    "    \n",
    "    if meta_model_type == 'random_forest':\n",
    "        meta_model = RandomForestRegressor(\n",
    "            n_estimators=trial.suggest_int('rf_n_estimators', 10, 100),\n",
    "            max_depth=trial.suggest_int('rf_max_depth', 2, 10),\n",
    "            min_samples_split=trial.suggest_int('rf_min_samples_split', 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int('rf_min_samples_leaf', 1, 10),\n",
    "            random_state=42\n",
    "        )\n",
    "    elif meta_model_type == 'ridge':\n",
    "        meta_model = Ridge(\n",
    "            alpha=trial.suggest_float('ridge_alpha', 0.1, 10.0),\n",
    "            random_state=42\n",
    "        )\n",
    "    else:  # gbm\n",
    "        meta_model = GradientBoostingRegressor(\n",
    "            n_estimators=trial.suggest_int('gbm_n_estimators', 10, 100),\n",
    "            learning_rate=trial.suggest_float('gbm_learning_rate', 0.01, 0.1),\n",
    "            max_depth=trial.suggest_int('gbm_max_depth', 2, 10),\n",
    "            min_samples_split=trial.suggest_int('gbm_min_samples_split', 2, 10),\n",
    "            min_samples_leaf=trial.suggest_int('gbm_min_samples_leaf', 1, 10),\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    # メタモデルのトレーニング\n",
    "    meta_model.fit(oof_preds.reshape(-1, 1), y_vals)\n",
    "    \n",
    "    # スタッキングによる予測\n",
    "    stacked_preds = meta_model.predict(oof_preds.reshape(-1, 1))\n",
    "    \n",
    "    # MAPEの計算\n",
    "    mape = mean_absolute_percentage_error(y_vals, stacked_preds)\n",
    "    \n",
=======
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "class StackingRegressor:\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=SEED)\n",
    "\n",
    "        # ベースモデルの予測値を保存するための空の配列\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "\n",
    "        # 各ベースモデルに対して学習と予測を行う\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "\n",
    "        # メタモデルを学習\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def catboost_params(trial):\n",
    "    return {\n",
    "        'depth': trial.suggest_int('cat_depth', 4, 16),\n",
    "        'learning_rate': trial.suggest_loguniform('cat_learning_rate', 0.00001, 0.1),\n",
    "        'iterations': trial.suggest_int('cat_iterations', 50, 2000),\n",
    "        'l2_leaf_reg': trial.suggest_int('cat_l2_leaf_reg', 1, 20),\n",
    "        'border_count': trial.suggest_categorical('cat_border_count', [32, 64, 128, 256, 512]),\n",
    "        'thread_count': 4\n",
    "    }\n",
    "\n",
    "def lightgbm_params(trial):\n",
    "    return {\n",
    "        'num_leaves': trial.suggest_int('lgb_num_leaves', 10, 50),\n",
    "        'max_depth': trial.suggest_int('lgb_max_depth', 2, 7),\n",
    "        'learning_rate': trial.suggest_float('lgb_learning_rate', 0.001, 0.1, log=True),\n",
    "        'n_estimators': trial.suggest_int('lgb_n_estimators', 50, 100),\n",
    "        'min_child_samples': trial.suggest_int('lgb_min_child_samples', 20, 100),\n",
    "        'subsample': trial.suggest_float('lgb_subsample', 0.5, 1.0, step=0.1),\n",
    "        'colsample_bytree': trial.suggest_float('lgb_colsample_bytree', 0.4, 1.0, step=0.1),\n",
    "        'reg_alpha': trial.suggest_float('lgb_reg_alpha', 0.0, 0.5, step=0.1),\n",
    "        'reg_lambda': trial.suggest_float('lgb_reg_lambda', 0.0, 0.5, step=0.1),\n",
    "        'force_col_wise': True,  \n",
    "        'verbose': -1  # 出力を無効にする\n",
    "    }\n",
    "\n",
    "# def xgboost_params(trial):\n",
    "#     return {\n",
    "#         'max_depth': trial.suggest_int('xgb_max_depth', 3, 9),\n",
    "#         'learning_rate': trial.suggest_loguniform('xgb_learning_rate', 0.001, 0.1),\n",
    "#         'n_estimators': trial.suggest_int('xgb_n_estimators', 50, 1000),\n",
    "#         'min_child_weight': trial.suggest_int('xgb_min_child_weight', 1, 10),\n",
    "#         'subsample': trial.suggest_float('xgb_subsample', 0.5, 1.0),\n",
    "#         'colsample_bytree': trial.suggest_float('xgb_colsample_bytree', 0.5, 1.0),\n",
    "#         'reg_alpha': trial.suggest_float('xgb_reg_alpha', 0.0, 0.5),\n",
    "#         'reg_lambda': trial.suggest_float('xgb_reg_lambda', 0.0, 0.5),\n",
    "#         'verbosity': 0,  # 出力を無効にする\n",
    "#         'objective': 'reg:squarederror',\n",
    "#         'booster': trial.suggest_categorical('xgb_booster', ['gbtree', 'gblinear', 'dart']),\n",
    "#         'tree_method': 'hist',\n",
    "#         'seed': 42\n",
    "#     }\n",
    "\n",
    "# def tabnet_params(trial):\n",
    "#     return {\n",
    "#         'n_d': trial.suggest_int('tab_n_d', 8, 64),\n",
    "#         'n_a': trial.suggest_int('tab_n_a', 8, 64),\n",
    "#         'n_steps': trial.suggest_int('tab_n_steps', 3, 10),\n",
    "#         'gamma': trial.suggest_float('tab_gamma', 1.0, 2.0),\n",
    "#         'n_independent': trial.suggest_int('tab_n_independent', 1, 5),\n",
    "#         'n_shared': trial.suggest_int('tab_n_shared', 1, 5),\n",
    "#         'lambda_sparse': trial.suggest_float('tab_lambda_sparse', 1e-5, 1e-1, log=True),\n",
    "#         'optimizer_fn': torch.optim.Adam,\n",
    "#         'scheduler_params': {\"step_size\":10, \"gamma\":0.9},\n",
    "#         'scheduler_fn': torch.optim.lr_scheduler.StepLR,\n",
    "#         'verbose': 0\n",
    "#     }\n",
    "\n",
    "def randomforest_params(trial):\n",
    "    return  {\n",
    "            'n_estimators': trial.suggest_int('rdf_n_estimators', 2, 150),\n",
    "            'max_depth': trial.suggest_int('rdf_max_depth', 1, 32, log=True),\n",
    "            'min_samples_split': trial.suggest_float('rdf_min_samples_split', 0.1, 1),\n",
    "            'min_samples_leaf': trial.suggest_float('rdf_min_samples_leaf', 0.1, 0.5),                \n",
    "            'max_features': trial.suggest_categorical('max_features', ['auto', 'sqrt', 'log2']),\n",
    "            'bootstrap': trial.suggest_categorical('rdf_bootstrap', [True, False]),\n",
    "            'random_state': 42\n",
    "    }\n",
    "    \n",
    "def gbm_params(trial):\n",
    "    return {\n",
    "        'n_estimators': trial.suggest_int('gbm_n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('gbm_max_depth', 3, 9),\n",
    "        'learning_rate': trial.suggest_loguniform('gbm_learning_rate', 0.001, 0.1),\n",
    "        'min_samples_split': trial.suggest_int('gbm_min_samples_split', 2, 10),\n",
    "        'min_samples_leaf': trial.suggest_int('gbm_min_samples_leaf', 1, 10),\n",
    "        'subsample': trial.suggest_float('gbm_subsample', 0.5, 1.0),\n",
    "        'max_features': trial.suggest_categorical('gbm_max_features', ['auto', 'sqrt', 'log2']),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "def ridge_params(trial):\n",
    "    return {\n",
    "        'alpha': trial.suggest_float('ridge_alpha', 0.001, 10.0, log=True),\n",
    "        'fit_intercept': trial.suggest_categorical('ridge_fit_intercept', [True, False]),\n",
    "        'normalize': trial.suggest_categorical('ridge_normalize', [True, False]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "def lasso_params(trial):\n",
    "    return {\n",
    "        'alpha': trial.suggest_float('lasso_alpha', 0.001, 10.0, log=True),\n",
    "        'fit_intercept': trial.suggest_categorical('lasso_fit_intercept', [True, False]),\n",
    "        'normalize': trial.suggest_categorical('lasso_normalize', [True, False]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    \n",
    "def objective(trial):\n",
    "    # メタモデルの選択\n",
    "    meta_model_name = trial.suggest_categorical('meta_model', ['lightgbm', 'catboost', 'randomforest', 'gbm', 'ridge', 'lasso']) #'xgboost',\n",
    "    \n",
    "    if meta_model_name == 'lightgbm':\n",
    "        meta_model = LGBMRegressor(**lightgbm_params(trial))\n",
    "    elif meta_model_name == 'catboost':\n",
    "        meta_model = CatBoostRegressor(**catboost_params(trial), verbose=0)\n",
    "    # elif meta_model_name == 'xgboost':\n",
    "    #     meta_model = xgb.XGBRegressor(**xgboost_params(trial))\n",
    "    elif meta_model_name == 'randomforest':\n",
    "        meta_model = RandomForestRegressor(**randomforest_params(trial))\n",
    "    elif meta_model_name == 'gbm':\n",
    "        meta_model = GradientBoostingRegressor(**gbm_params(trial))\n",
    "    elif meta_model_name == 'ridge':\n",
    "        meta_model = Ridge(**ridge_params(trial))\n",
    "    elif meta_model_name == 'lasso':\n",
    "        meta_model = Lasso(**lasso_params(trial))\n",
    "        \n",
    "    # ベースモデルの定義\n",
    "    base_models = []\n",
    "    \n",
    "    if meta_model_name != 'lightgbm':\n",
    "        base_models.append(LGBMRegressor(**lightgbm_params(trial)))\n",
    "    if meta_model_name != 'catboost':\n",
    "        base_models.append(CatBoostRegressor(**catboost_params(trial), verbose=0))\n",
    "    # if meta_model_name != 'xgboost':\n",
    "    #     base_models.append(xgb.XGBRegressor(**xgboost_params(trial)))\n",
    "    if meta_model_name != 'randomforest':\n",
    "        base_models.append(RandomForestRegressor(**randomforest_params(trial)))\n",
    "    if meta_model_name != 'gbm':\n",
    "        base_models.append(GradientBoostingRegressor(**gbm_params(trial)))\n",
    "    if meta_model_name != 'ridge':\n",
    "        base_models.append(Ridge(**ridge_params(trial)))\n",
    "    if meta_model_name != 'lasso':\n",
    "        base_models.append(Lasso(**lasso_params(trial)))\n",
    "    \n",
    "    stacking_regressor = StackingRegressor(base_models=base_models, meta_model=meta_model, n_folds=3)\n",
    "    stacking_regressor.fit(X_train.values, y_train.values)\n",
    "    stacked_pred = stacking_regressor.predict(X_val.values)\n",
    "\n",
    "    # 評価\n",
    "    mape = mean_absolute_percentage_error(y_val, stacked_pred)\n",
>>>>>>> 35f3b5f (update)
    "    return mape"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-12 16:04:22,468] A new study created in memory with name: no-name-7440ce14-561a-4abc-abe1-9b4fc683dd9c\n",
      "[I 2023-08-12 16:04:23,005] Trial 0 finished with value: 0.6933914596429692 and parameters: {'model_type': 'xgboost', 'max_depth': 8, 'learning_rate': 0.0825864670684569, 'n_estimators': 211, 'min_child_weight': 7, 'subsample': 0.7142500766353812, 'colsample_bytree': 0.9906135485711739, 'reg_alpha': 0.43156230610011287, 'reg_lambda': 0.085552341124279, 'booster': 'dart', 'meta_model_type': 'ridge', 'ridge_alpha': 6.026718993550662}. Best is trial 0 with value: 0.6933914596429692.\n",
      "[W 2023-08-12 16:12:13,783] Trial 1 failed with parameters: {'model_type': 'catboost', 'depth': 15, 'learning_rate': 0.002537815508265664, 'iterations': 1431, 'l2_leaf_reg': 1, 'border_count': 32} because of the following error: KeyboardInterrupt('').\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_231326/1129206442.py\", line 26, in objective\n",
      "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10,\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/catboost/core.py\", line 5734, in fit\n",
      "    return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/catboost/core.py\", line 2357, in _fit\n",
      "    self._train(\n",
      "  File \"/usr/local/conda/lib/python3.9/site-packages/catboost/core.py\", line 1761, in _train\n",
      "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
      "  File \"_catboost.pyx\", line 4624, in _catboost._CatBoost._train\n",
      "  File \"_catboost.pyx\", line 4673, in _catboost._CatBoost._train\n",
      "KeyboardInterrupt\n",
      "[W 2023-08-12 16:12:13,787] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_231326/3544081241.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 最適化を実行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9000\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 合計のトライアル数とタイムアウトを調整\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best trial:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    440\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \"\"\"\n\u001b[0;32m--> 442\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    443\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     ):\n\u001b[0;32m--> 251\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_231326/1129206442.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     24\u001b[0m             }\n\u001b[1;32m     25\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, \n\u001b[0m\u001b[1;32m     27\u001b[0m                       cat_features=['region', 'manufacturer', 'condition', 'cylinders', 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state'])\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   5732\u001b[0m             \u001b[0mCatBoostRegressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_is_compatible_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss_function'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5734\u001b[0;31m         return self._fit(X, y, cat_features, text_features, embedding_features, None, sample_weight, None, None, None, None, baseline,\n\u001b[0m\u001b[1;32m   5735\u001b[0m                          \u001b[0muse_best_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogging_level\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn_description\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5736\u001b[0m                          \u001b[0mverbose_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric_period\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mlog_fixup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_cout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_cerr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m             \u001b[0mplot_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Training plots'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_get_train_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2357\u001b[0;31m             self._train(\n\u001b[0m\u001b[1;32m   2358\u001b[0m                 \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m                 \u001b[0mtrain_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_sets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/conda/lib/python3.9/site-packages/catboost/core.py\u001b[0m in \u001b[0;36m_train\u001b[0;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[1;32m   1759\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1760\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1761\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_clear_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_object\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_model\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_trained_model_attributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m_catboost.pyx\u001b[0m in \u001b[0;36m_catboost._CatBoost._train\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
=======
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be0aed92ee054e98b559da17fee7cccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
>>>>>>> 35f3b5f (update)
    }
   ],
   "source": [
    "# Optunaのスタディを作成\n",
<<<<<<< HEAD
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "\n",
    "# 初期のトライアルを追加\n",
    "study.enqueue_trial({'model_type': 'xgboost', 'max_depth': 8, 'learning_rate': 0.0825864670684569, 'n_estimators': 211, 'min_child_weight': 7, 'subsample': 0.7142500766353812, 'colsample_bytree': 0.9906135485711739, 'reg_alpha': 0.43156230610011287, 'reg_lambda': 0.085552341124279, 'booster': 'dart'})\n",
    "\n",
    "# 最適化を実行\n",
    "study.optimize(objective, n_trials=5, timeout=9000)  # 合計のトライアル数とタイムアウトを調整\n",
    "\n",
    "print('Best trial:', study.best_trial.params)"
=======
    "study = optuna.create_study(direction='minimize', \n",
    "                            pruner=optuna.pruners.MedianPruner(n_warmup_steps=30), \n",
    "                            sampler=optuna.samplers.TPESampler(seed=SEED))\n",
    "\n",
    "# 初期のトライアルを追加\n",
    "# study.enqueue_trial({'meta_model': 'xgboost', 'xgb_max_depth': 7, 'xgb_learning_rate': 0.007298529657272836, 'xgb_n_estimators': 451, 'xgb_min_child_weight': 6, 'xgb_subsample': 0.9958848223947634, 'xgb_colsample_bytree': 0.8842597694415, 'xgb_reg_alpha': 0.1176968294395841, 'xgb_reg_lambda': 0.11135946154846349, 'xgb_booster': 'dart', 'lgb_num_leaves': 34, 'lgb_max_depth': 2, 'lgb_learning_rate': 0.08905902613718662, 'lgb_n_estimators': 86, 'lgb_min_child_samples': 61, 'lgb_subsample': 0.9, 'lgb_colsample_bytree': 0.6000000000000001, 'lgb_reg_alpha': 0.0, 'lgb_reg_lambda': 0.30000000000000004, 'cat_depth': 7, 'cat_learning_rate': 0.025078620807271634, 'cat_iterations': 1417, 'cat_l2_leaf_reg': 9, 'cat_border_count': 512, 'rdf_n_estimators': 49, 'rdf_max_depth': 10, 'rdf_min_samples_split': 0.5824858478687621, 'rdf_min_samples_leaf': 0.4543824500835886, 'max_features': 'sqrt', 'rdf_bootstrap': False, 'gbm_n_estimators': 210, 'gbm_max_depth': 4, 'gbm_learning_rate': 0.02366888487010509, 'gbm_min_samples_split': 3, 'gbm_min_samples_leaf': 6, 'gbm_subsample': 0.969132943790793, 'gbm_max_features': 'auto', 'ridge_alpha': 0.0020670461152815763, 'ridge_fit_intercept': False, 'ridge_normalize': False, 'lasso_alpha': 0.2647912837509226, 'lasso_fit_intercept': False, 'lasso_normalize': False})\n",
    "\n",
    "# 最適化を実行  \n",
    "study.optimize(objective, \n",
    "               n_trials=100, \n",
    "               timeout=None, \n",
    "               n_jobs=3, \n",
    "               gc_after_trial=False, \n",
    "               show_progress_bar=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'study' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_125383/2100856189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best trial score: {study.best_value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best trial params: {study.best_params}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'study' is not defined"
     ]
    }
   ],
   "source": [
    "print(f\"Best trial score: {study.best_value}\")\n",
    "print(f\"Best trial params: {study.best_params}\")"
>>>>>>> 35f3b5f (update)
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適なモデルの取得\n",
    "best_params = study.best_trial.params\n",
    "meta_model_type = best_params['meta_model_type']\n",
    "del best_params['meta_model_type']  # メタモデルタイプをパラメータから削除\n",
    "\n",
    "model_name = \"meta_model\"\n",
    "\n",
    "if meta_model_type == 'random_forest':\n",
    "    best_meta_model = RandomForestRegressor(\n",
    "        n_estimators=best_params['rf_n_estimators'],\n",
    "        max_depth=best_params['rf_max_depth'],\n",
    "        min_samples_split=best_params['rf_min_samples_split'],\n",
    "        min_samples_leaf=best_params['rf_min_samples_leaf'],\n",
<<<<<<< HEAD
=======
    "        bootstrap=best_params['rf_bootstrap'],\n",
>>>>>>> 35f3b5f (update)
    "        random_state=42\n",
    "    )\n",
    "    best_meta_model.fit(oof_preds.reshape(-1, 1), y_vals)\n",
    "    # モデルを保存\n",
    "    with open(f'../model/{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(best_meta_model, f)\n",
    "\n",
    "elif meta_model_type == 'ridge':\n",
    "    best_meta_model = Ridge(alpha=best_params['ridge_alpha'], random_state=42)\n",
    "    best_meta_model.fit(oof_preds.reshape(-1, 1), y_vals)\n",
    "    # モデルを保存\n",
    "    with open(f'../model/{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(best_meta_model, f)\n",
    "\n",
    "elif meta_model_type == 'gbm':\n",
    "    best_meta_model = GradientBoostingRegressor(\n",
    "        n_estimators=best_params['gbm_n_estimators'],\n",
    "        learning_rate=best_params['gbm_learning_rate'],\n",
    "        max_depth=best_params['gbm_max_depth'],\n",
    "        min_samples_split=best_params['gbm_min_samples_split'],\n",
    "        min_samples_leaf=best_params['gbm_min_samples_leaf'],\n",
<<<<<<< HEAD
    "        random_state=42\n",
    "    )\n",
    "    best_meta_model.fit(oof_preds.reshape(-1, 1), y_vals)\n",
=======
    "        subsample=best_params['gbm_subsample'],\n",
    "        random_state=42\n",
    "    )\n",
    "    best_meta_model.fit(oof_preds.reshape(-1, 1), y_vals)\n",
    "    \n",
    "    \n",
>>>>>>> 35f3b5f (update)
    "    # モデルを保存\n",
    "    with open(f'../model/{model_name}.pkl', 'wb') as f:\n",
    "        pickle.dump(best_meta_model, f)\n"
   ]
<<<<<<< HEAD
=======
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適なモデルの取得\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "# モデルの選択\n",
    "model_type = best_params['model_type']\n",
    "del best_params['meta_model_type']  # メタモデルタイプをパラメータから削除\n",
    "    \n",
    "# 交差検証の設定\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "oof_preds = []\n",
    "y_vals = []\n",
    "val_preds = []  # ホールドアウトセットの予測を保存するためのリスト\n",
    "    \n",
    "for train_idx, val_idx in kf.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "    # ホールドアウトセットの作成\n",
    "    X_train, X_holdout, y_train, y_holdout = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "        \n",
    "    if model_type == 'catboost':\n",
    "        cat_params = {\n",
    "            'depth': best_params['cat_depth'],\n",
    "            'learning_rate': best_params['cat_learning_rate'],\n",
    "            'iterations': best_params['cat_iterations'],\n",
    "            'l2_leaf_reg': best_params['cat_l2_leaf_reg'],\n",
    "            'border_count': best_params['cat_border_count'],\n",
    "            'thread_count': 4\n",
    "        }\n",
    "        cat_model = CatBoostRegressor(**cat_params, random_seed=42, verbose=0)\n",
    "        cat_model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=10, \n",
    "                    cat_features=['region', 'manufacturer', 'condition', 'cylinders', 'fuel', 'title_status', 'transmission', 'drive', 'size', 'type', 'paint_color', 'state'])\n",
    "        \n",
    "    elif model_type == 'lightgbm':\n",
    "       lgb_params = {\n",
    "            'num_leaves': best_params['lgb_num_leaves'],\n",
    "            'max_depth': best_params['lgb_max_depth'],\n",
    "            'learning_rate': best_params['lgb_learning_rate'],\n",
    "            'n_estimators': best_params['lgb_n_estimators'],\n",
    "            'min_child_samples': best_params['lgb_min_child_samples'],\n",
    "            'subsample': best_params['lgb_subsample'],\n",
    "            'colsample_bytree': best_params['lgb_colsample_bytree'],\n",
    "            'reg_alpha': best_params['lgb_reg_alpha'],\n",
    "            'reg_lambda': best_params['lgb_reg_lambda'],\n",
    "            'force_col_wise': True,\n",
    "            'verbose': -1  # 出力を無効にする\n",
    "        }\n",
    "        lgb_model = LGBMRegressor(**lgb_params, random_state=42)\n",
    "        lgb_model.fit(X_train, y_train, eval_metric='mape', eval_set=[(X_val, y_val)]) # 出力を無効にする\n",
    "            \n",
    "    else:  # xgboost\n",
    "        xgb_params = {\n",
    "            'max_depth': best_params['xgb_max_depth'],\n",
    "            'learning_rate': best_params['xgb_learning_rate'],\n",
    "            'n_estimators': best_params['xgb_n_estimators'],\n",
    "            'min_child_weight': best_params['xgb_min_child_weight'],\n",
    "            'subsample': best_params['xgb_subsample'],\n",
    "            'colsample_bytree': best_params['xgb_colsample_bytree'],\n",
    "            'reg_alpha': best_params['xgb_reg_alpha'],\n",
    "            'reg_lambda': best_params['xgb_reg_lambda'],\n",
    "            'verbosity': 0,  # 出力を無効にする\n",
    "            'objective': 'reg:squarederror',\n",
    "            'booster': best_params['xgb_booster'],\n",
    "            'tree_method': 'hist',\n",
    "            'seed': 42\n",
    "        }\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        xgb_model = xgb.train(xgb_params, dval, evals=[(dval, 'eval')], early_stopping_rounds=10, verbose_eval=False)\n",
    "        \n",
    "    # モデルの予測\n",
    "    if model_type == 'catboost':\n",
    "            preds = cat_model.predict(xgb.DMatrix(X_val))\n",
    "            holdout_preds = cat_model.predict(xgb.DMatrix(X_holdout))\n",
    "    elif model_type == 'lghtgbm':\n",
    "            preds = lgb_model.predict(X_val)\n",
    "            holdout_preds = lgb_model.predict(X_holdout)\n",
    "    else:\n",
    "            preds = xgb_model.predict(X_val)\n",
    "            holdout_preds = xgb_model.predict(X_holdout)\n",
    "\n",
    "\n",
    "    oof_preds.append(preds)\n",
    "    y_vals.append(y_val)\n",
    "    \n",
    "    # OOF予測をスタッキングの入力として使用\n",
    "    oof_preds.append(preds)\n",
    "    y_vals.append(y_val)\n",
    "    val_preds.append(holdout_preds)\n",
    "    \n",
    "# OOF予測とホールドアウトセットの予測をスタッキングの入力として使用\n",
    "oof_preds = np.concatenate(oof_preds, axis=0)\n",
    "y_vals = np.concatenate(y_vals, axis=0)\n",
    "val_preds = np.concatenate(val_preds, axis=0)\n",
    "    \n",
    "# メタモデルの選択\n",
    "meta_model_type = best_params['meta_model_type']\n",
    "    \n",
    "if meta_model_type == 'random_forest':\n",
    "    meta_model = RandomForestRegressor(\n",
    "        n_estimators=best_params['rf_n_estimators'],\n",
    "        max_depth=best_params['rf_max_depth'],\n",
    "        min_samples_split=best_params['rf_min_samples_split'],\n",
    "        min_samples_leaf=best_params['rf_min_samples_leaf'],\n",
    "        random_state=42\n",
    "    )\n",
    "elif meta_model_type == 'ridge':\n",
    "    meta_model = Ridge(\n",
    "        alpha=best_params['ridge_alpha'],\n",
    "        random_state=42\n",
    "    )\n",
    "else:  # gbm\n",
    "    meta_model = GradientBoostingRegressor(\n",
    "        n_estimators=best_params['gbm_n_estimators'],\n",
    "        learning_rate=best_params['gbm_learning_rate'],\n",
    "        max_depth=best_params['gbm_max_depth'],\n",
    "        min_samples_split=best_params['gbm_min_samples_split'],\n",
    "        min_samples_leaf=best_params['gbm_min_samples_leaf'],\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "# メタモデルのトレーニング\n",
    "meta_model.fit(oof_preds.reshape(-1, 1), y_vals)"
   ]
>>>>>>> 35f3b5f (update)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
